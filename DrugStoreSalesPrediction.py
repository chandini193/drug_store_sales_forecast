# -*- coding: utf-8 -*-
"""Projec_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hiaYvlnBlreuY9qxM3jO6cixlD9ukncy

# Declarative section
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from pandas import datetime


#for visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# time series analysis
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import  ARIMA
import statsmodels.api as sm


#For generating cartesian product
import itertools

#For ignoring warnings
import warnings 
warnings.filterwarnings('ignore')

#for model creation
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

"""# Mounting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Reading dataset

"""

train = pd.read_csv("/content/drive/My Drive/Dataset_project_3/train.csv",parse_dates = True, index_col = 'Date')

store = pd.read_csv("/content/drive/My Drive/Dataset_project_3/store.csv")

state = pd.read_csv("/content/drive/My Drive/Dataset_project_3/state.csv")

"""## Extraction of Data"""

train['Year'] = train.index.year
train['Month'] = train.index.month
train['Day'] = train.index.day

# adding new variable
train['SalePerCustomer'] = train['Sales']/train['Customers']

"""# Displaying dataset"""

train.head()

state.head()

store.head()

"""# Data Description"""

store.describe()

train.describe()

state.describe()

"""# Checking null values

"""

train.isnull().sum()

store.isnull().sum()

store['StoreType'].value_counts() 
# Obeservation :  Store b is very less in count comapre to a,c,d

state.isnull().sum()

"""# Filling Missing values"""

# Closed stores and zero sales stores
train[(train.Open == 0) & (train.Sales == 0)].head()

# opened stores with zero sales
train[(train.Open != 0) & (train.Sales == 0)].head()

# Removing closed stores with zero sales
train = train[(train["Open"] != 0) & (train['Sales'] != 0)]

train.isnull().sum()

# missing values in CompetitionDistance
store[pd.isnull(store.CompetitionDistance)]

# fill NaN with a median value
store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)

store.loc[290]

store.loc[621]

store.loc[878]

# Replacing remaining all null values with 0
store.fillna(0, inplace = True)

store.isnull().sum()

"""# Merging two datasets store and train"""

# merging two datasets using inner join to identify how sales is influenced by other parameters
train_store = pd.merge(train, store, how = 'inner', on = 'Store')

train_store_state = pd.merge(train_store, state, how='inner', on ='Store');

train_store_state.shape

train_store_state.head(50)

train_store_state.iloc[915:960]

train_store_state.groupby('StoreType')['Sales'].describe()  # checking sales details on each store type

# Observation : count of store b is less but the average sales is very high when compared to others.

train_store_state.groupby('StoreType')['Customers'].sum()
# Observation : store type a has highest number of customers
# Observation : store b have less number of customers but have high average sales

"""# Correalation"""

plt.figure(figsize=(10,8))
sns.heatmap(train_store.corr())

# Observation : sales is highly correalted to Customers, Open, Promo

"""# Plots"""

fig,ax=plt.subplots(2,2)
train_store_state.plot(kind='scatter',x='Customers',y='Sales',ax=ax[0][0],figsize=(16,8))
train_store_state.plot(kind='scatter',x='Open',y='Sales',ax=ax[0][1])
train_store_state.plot(kind='scatter',x='Promo',y='Sales',ax=ax[1][0])
plt.show()

# Sales with promotion and without promotion at different months
sns.factorplot(data = train_store_state, x = 'Month', y = "Sales", 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo', # per promo in the store in rows
               col_order = ['a','b','c','d']
               ) 

# Observation : sales with promotion is high when compared to without promotion
# Observation : sales in december month is high

# Sales at different DayOfWeek
sns.factorplot(data = train_store_state, x = 'DayOfWeek', y = "Sales", 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               col_order = ['a','b','c','d']
               ) 
# Observation : on 7th i.e weekend day the sales in decreasing in a,c and d but not in store of type b

# Sales at different DayOfWeek of different months
sns.factorplot(data = train_store_state, x = 'Month', y = "Sales", 
               hue = 'StoreType',
               col = 'DayOfWeek', # per day  in cols
               row = 'StoreType', # per store type in rows
               row_order = ['a','b','c','d']
               ) 
# Observation : Store type c are closed on sundays
# Observation : Store type d are closed from october to december on sundays

# Customers with promotion and without promotion at different months
sns.factorplot(data = train_store_state, x = 'Month', y = "Customers", 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo', # per promo in the store in rows
               col_order = ['a','b','c','d']
               ) 
# Observation : more number of customers visiting to store type b when compared to others
# Observation : the number of customers visiting is increased with promotion
# Observation : In the month of december the sales is high in almost all stores

#On an avreage SalePerCustomer of each month
sns.factorplot(data = train_store_state, x = 'Month', y = 'SalePerCustomer', 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo', # per promo in the store in rows
               col_order = ['a','b','c','d']
               ) 
# Observation : Even though the sales and the no.of customers visiting StoreType b are more but the SalesPerCustomer is very less when compare to others.
# Means these are like retail shops.

# Sales with Promo2 and without Promo2 at different months
sns.factorplot(data = train_store_state, x = 'Month', y = 'Sales', 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo2', # per promo2 in the store in rows
               col_order = ['a','b','c','d']
               ) 
# Observation : sales without longterm promotions is high

# Customers with Promo2 and with out Promo2 at different months
sns.factorplot(data = train_store_state, x = 'Month', y = "Customers", 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo2', # per promo in the store in rows
               col_order = ['a','b','c','d']
               ) 
# Observation : When there is longterm promotions the customers visting to stores is more but sales is less

#On an avreage SalesPerCustomer of each month
sns.factorplot(data = train_store_state, x = 'Month', y = 'SalePerCustomer', 
               hue = 'StoreType',
               col = 'StoreType', # per store type in cols
               row = 'Promo2', # per promo2 in the store in rows
               col_order = ['a','b','c','d']
               ) 
# Observation :  In 7 month sales is high in all other stores except in store b
# Observation : When there is long term promotion SalsePerCustomer is increasing

fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,4))

sns.barplot(x='SchoolHoliday', y='Sales', data=train_store_state, ax=axis1)
sns.barplot(x='SchoolHoliday', y='Customers', data=train_store_state, ax=axis2)
sns.barplot(x='SchoolHoliday',y='SalePerCustomer',data=train_store_state,ax=axis3)
# Observation : Sales,Customers,SalePerCustomer are high when there is school holiday compared to workingdays

train_store_state['StateHoliday'] = train_store_state.StateHoliday.astype(str)

fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,4))

sns.barplot(x='StateHoliday', y='Sales', data=train_store_state, ax=axis1)
sns.barplot(x='StateHoliday', y='Customers', data=train_store_state, ax=axis2)
sns.barplot(x='StateHoliday',y='SalePerCustomer',data=train_store_state,ax=axis3)
# Observation : Here a,b,c are public and festival holidays, in this time sales are high.

sns.countplot(x="StoreType",hue="Assortment",order=["a","b","c","d"], data=train_store_state).set_title("Number of Different Assortments per Store Type")
# Observation : assortment type b is only available in store b
# Observation : assortment type a is more in store a

# sales per customer trends
sns.factorplot(data = train_store, x = 'DayOfWeek', y = 'Sales', 
               col = 'Promo', 
               row = 'Promo2',
               hue = 'Promo2',
              ) 

# Observation : No promotion on weekends
# Observation : Promo and Promo2 are equal to 0, Sales tend to peak on Sunday. We know StoreType C doesn't work on Sundays. So it is mainly data from StoreType A, B and D.
# Observation : stores that run the promotion tend to make most of the Sales on Monday. This fact could be a good indicator for Rossmann marketing campaigns. The same trend follow the stores which have both promotion at the same time.
# Observation : Promo2 alone doesn't seem to be correlated to any significant change in the Sales amount.

# Sales trend over the months and year
sns.factorplot(data = train_store_state, x ='Month', y = 'Sales', 
               col = 'Promo', # per store type in cols
               hue = 'Promo2',
               row = 'Year')
# Observation : Sales tend to spike in December, which makes sense because of the Christmas and holiday season. So, this confirms that the sales vary with the ‘Date’ (time) and there is a seasonality factor present in our data.

"""## Model

# ARIMA - AutoRegressive Integrated Moving Averages

process of ARIMA model:
1. Visualize time series data
2. Make the time series data stationary i.e without any trends
3. Plot correlation and auto correlation charts for identifying lags
4. Construct SARIMA model
5. Use the model to make predictions

## Checking stationarity

In order to use time series forecasting models, we need to ensure that our time series data is stationary i.e constant mean, constant variance and constant covariance with time i.e no seasonality.
"""

# preparation: input should be float type
train['Sales'] = train['Sales'] * 1.0

# store types
sales_a = train[train.Store == 2]['Sales']
sales_b = train[train.Store == 85]['Sales'].sort_index(ascending = True) # sort the reverse order
sales_c = train[train.Store == 1]['Sales']
sales_d = train[train.Store == 13]['Sales']

f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))

# store types resampling form days to weeks to see tends more clearly
sales_a.resample('W').sum().plot(ax = ax1)
sales_b.resample('W').sum().plot(ax = ax2)
sales_c.resample('W').sum().plot(ax = ax3)
sales_d.resample('W').sum().plot(ax = ax4)

"""## Augmented Dicky-Fuller test
H0 : Time series is non-stationary.

H1 : Time series is stationary.

So, if the p-value of the test is less than the significance level (< 0.05) then reject the null hypothesis and infer that the time series is indeed stationary.
"""

print('Results of Dickey-Fuller Test:')
result = adfuller(sales_a, autolag='AIC')
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print(key, value)

print('Results of Dickey-Fuller Test:')
result = adfuller(sales_b, autolag='AIC')
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print(key, value)

print('Results of Dickey-Fuller Test:')
result = adfuller(sales_c, autolag='AIC')
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print(key, value)

print('Results of Dickey-Fuller Test:')
result = adfuller(sales_d, autolag='AIC')
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print(key, value)

"""## Auto Correlation & Partial Auto Correlation

AC =  Direct + Indirect effect of lags on today day sales.
Example : today salses is effected by yesterday and day before yesterday directly. And day before yesterday's imapact on yesterday effectes today sales indirectly. 

PAC = Direct effect of lags on todays sales.

For Auto Regression i.e p we use PACF

For Moving Average i.e q we use ACF
"""

# figure for subplots
plt.figure(figsize = (12, 8))

# acf and pacf for A
plt.subplot(4,2,1) # four rows, each row 2 columns, and the position of this graph
plot_acf(sales_a, lags = 50, ax = plt.gca())
plt.subplot(4,2,2)
plot_pacf(sales_a, lags = 50, ax = plt.gca())

# acf and pacf for B
plt.subplot(4,2,3)
plot_acf(sales_b, lags = 50, ax = plt.gca())
plt.subplot(4,2,4)
plot_pacf(sales_b, lags = 50, ax = plt.gca())

# acf and pacf for C
plt.subplot(4,2,5)
plot_acf(sales_c, lags = 50, ax = plt.gca())
plt.subplot(4,2,6)
plot_pacf(sales_c, lags = 50, ax = plt.gca())

# acf and pacf for D
plt.subplot(4,2,7)
plot_acf(sales_d, lags = 50, ax = plt.gca())
plt.subplot(4,2,8)
plot_pacf(sales_d, lags = 50, ax = plt.gca())

plt.show()

p = d = q = range(0, 2)
pdq = list(itertools.product(p, d, q)) #computes the cartesian product of input iterables i.e all possibe combinatios of p, d, q between 0 and 2
pdq

seasonal_pdq = [(x[0], x[1], x[2], 12) for x in pdq] # For seasonal parameters P, D, Q
seasonal_pdq

"""# Hyper Parameter Tuning
The best combination of parameters will give the lowest Akaike information criterion (AIC) score. AIC tells us the quality of statistical models for a given set of data.
"""

train = pd.read_csv("/content/drive/My Drive/Dataset_project_3/train.csv",parse_dates = True, index_col = 'Date')
y = train['Sales'].resample('W').mean()
y.shape

import warnings 
warnings.filterwarnings('ignore')
# Determing p,d,q combinations with AIC scores.
for param in pdq:
    for param_seasonal in seasonal_pdq:
            model = sm.tsa.statespace.SARIMAX(y,order=param,seasonal_order=param_seasonal,enforce_stationarity=False,enforce_invertibility=False)
            results = model.fit()
            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))

import warnings 
warnings.filterwarnings('ignore')

# Fitting the data to ARIMA model 
model_sarima = sm.tsa.statespace.SARIMAX(y,
                                order=(1, 1, 1),
                                seasonal_order=(0, 1, 1, 12),
                                enforce_stationarity=False,
                                enforce_invertibility=False)
results_sarima = model_sarima.fit()
print(results_sarima.summary().tables[1])

results_sarima.plot_diagnostics(figsize=(15, 12))
plt.show()

# Model Prediction

pred = results.get_prediction(start=pd.to_datetime('2015-01-11'), dynamic=False) # Predictions are performed for the 11th Jan 2015 onwards of the train data.
#The dynamic=False argument ensures that forecasts at each point are generated using the full history up to that point.
pred_ci = pred.conf_int() # Get confidence intervals of forecasts


ax = y['2014':].plot(label='observed',figsize=(15, 7))
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)

ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)

ax.set_xlabel('Date')
ax.set_ylabel('Sales')
plt.legend()

plt.show()

y_forecasted = pred.predicted_mean
y_truth = y['2015-01-11':]

# Compute the mean square error
mse = ((y_forecasted - y_truth) ** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))

pred_dynamic = results.get_prediction(start=pd.to_datetime('2015-01-11'), dynamic=True, full_results=True) 
#Dynamic forecast uses the value of the previous forecasted value of the dependent variable to compute the next one.
pred_dynamic_ci = pred_dynamic.conf_int()

ax = y['2014':].plot(label='observed', figsize=(20, 15))
pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)

ax.fill_between(pred_dynamic_ci.index,
                pred_dynamic_ci.iloc[:, 0],
                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)

ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2015-01-11'), y.index[-1],
                 alpha=.1, zorder=-1)

ax.set_xlabel('Date')
ax.set_ylabel('Sales')

plt.legend()
plt.show()

# Extract the predicted and true values of our time series
y_forecasted = pred_dynamic.predicted_mean
y_truth = y['2015-01-11':]

# Compute the mean square error
mse = ((y_forecasted - y_truth) ** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))

# Get forecast 6 steps i.e 6 weeks ahead in future
pred_uc = results.get_forecast(steps=6)

# Get confidence intervals of forecasts
pred_ci = pred_uc.conf_int()

ax = y.plot(label='observed', figsize=(20, 15))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.25)
ax.set_xlabel('Date')
ax.set_ylabel('Sales')

plt.legend()
plt.show()